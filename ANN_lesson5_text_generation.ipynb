{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hupokLFNdM5b",
    "outputId": "9efe24a1-f686-4bd0-f2b6-a596f94705b4"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.utils import pad_sequences, to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Dropout, Activation\n",
    "from keras.datasets import imdb\n",
    "from tensorflow.keras.layers import LSTM, SimpleRNN, GRU\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHrx7HnL2XFU"
   },
   "source": [
    "2. Попробуйте изменить параметры нейронной сети генерирующий текст таким образом, чтобы добиться генерации как можно более осмысленного текста. Пришлите лучший получившейся у вас текст и опишите, что вы предприняли, чтобы его получить. Можно использовать текст другого прозведения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1pzs6ciqB7kU"
   },
   "outputs": [],
   "source": [
    "filename = \"wonderland.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "7-6doI8_BUb3"
   },
   "outputs": [],
   "source": [
    "with open(filename, 'rb') as _in:\n",
    "    lines = []\n",
    "    for line in _in:\n",
    "        line = line.strip().lower().decode(\"ascii\", \"ignore\")\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        lines.append(line)\n",
    "text = \" \".join(lines)\n",
    "chars = set([c for c in text])\n",
    "nb_chars = len(chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8XOjDByCMxE",
    "outputId": "7b45ec19-ce9f-43b4-9ca1-44c04022b03d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "jVESFm0LE6HV"
   },
   "outputs": [],
   "source": [
    "char2index = {c: i for i, c in enumerate(chars)}\n",
    "index2char = {i: c for i, c in enumerate(chars)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "xzEKZ9fvE-dx"
   },
   "outputs": [],
   "source": [
    "SEQLEN, STEP = 100, 1\n",
    "input_chars, label_chars = [], []\n",
    "\n",
    "for i in range(0, len(text) - SEQLEN, STEP):\n",
    "    input_chars.append(text[i: i + SEQLEN])\n",
    "    label_chars.append(text[i + SEQLEN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VpY8kbQAFFzM",
    "outputId": "0bd3bb88-6850-496c-f682-fee6c0398479"
   },
   "outputs": [],
   "source": [
    "X = np.zeros((len(input_chars), SEQLEN, nb_chars), dtype=bool)\n",
    "y = np.zeros((len(input_chars), nb_chars), dtype=bool)\n",
    "for i, input_char in enumerate(input_chars):\n",
    "    for j, ch in enumerate(input_char):\n",
    "        X[i, j, char2index[ch]] = 1\n",
    "    y[i, char2index[label_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "KHHF_hetI8rp"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE, HIDDEN_SIZE = 128, 128\n",
    "NUM_ITERATIONS = 5 \n",
    "NUM_EPOCHS_PER_ITERATION = 25\n",
    "NUM_PREDS_PER_EPOCH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "220hxsj1JIPk",
    "outputId": "3e5af424-12ca-42dd-b3da-cbf1edea005b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Итерация #: 0\n",
      "Epoch 1/25\n",
      "1095/1095 [==============================] - 67s 43ms/step - loss: 2.2826\n",
      "Epoch 2/25\n",
      "1095/1095 [==============================] - 47s 43ms/step - loss: 1.8941\n",
      "Epoch 3/25\n",
      "1095/1095 [==============================] - 52s 47ms/step - loss: 1.7462\n",
      "Epoch 4/25\n",
      "1095/1095 [==============================] - 48s 44ms/step - loss: 1.6417\n",
      "Epoch 5/25\n",
      "1095/1095 [==============================] - 48s 43ms/step - loss: 1.5609\n",
      "Epoch 6/25\n",
      "1095/1095 [==============================] - 47s 43ms/step - loss: 1.4970\n",
      "Epoch 7/25\n",
      "1095/1095 [==============================] - 47s 43ms/step - loss: 1.4443\n",
      "Epoch 8/25\n",
      "1095/1095 [==============================] - 50s 45ms/step - loss: 1.4002\n",
      "Epoch 9/25\n",
      "1095/1095 [==============================] - 47s 43ms/step - loss: 1.3628\n",
      "Epoch 10/25\n",
      "1095/1095 [==============================] - 46s 42ms/step - loss: 1.3292\n",
      "Epoch 11/25\n",
      "1095/1095 [==============================] - 46s 42ms/step - loss: 1.2997\n",
      "Epoch 12/25\n",
      "1095/1095 [==============================] - 47s 43ms/step - loss: 1.2726\n",
      "Epoch 13/25\n",
      "1095/1095 [==============================] - 46s 42ms/step - loss: 1.2491\n",
      "Epoch 14/25\n",
      "1095/1095 [==============================] - 50s 45ms/step - loss: 1.2273\n",
      "Epoch 15/25\n",
      "1095/1095 [==============================] - 47s 43ms/step - loss: 1.2072\n",
      "Epoch 16/25\n",
      "1095/1095 [==============================] - 46s 42ms/step - loss: 1.1887\n",
      "Epoch 17/25\n",
      "1095/1095 [==============================] - 46s 42ms/step - loss: 1.1707\n",
      "Epoch 18/25\n",
      "1095/1095 [==============================] - 46s 42ms/step - loss: 1.1551\n",
      "Epoch 19/25\n",
      "1095/1095 [==============================] - 46s 42ms/step - loss: 1.1394\n",
      "Epoch 20/25\n",
      "1095/1095 [==============================] - 50s 45ms/step - loss: 1.1245\n",
      "Epoch 21/25\n",
      "1095/1095 [==============================] - 46s 42ms/step - loss: 1.1112\n",
      "Epoch 22/25\n",
      "1095/1095 [==============================] - 46s 42ms/step - loss: 1.0978\n",
      "Epoch 23/25\n",
      "1095/1095 [==============================] - 46s 42ms/step - loss: 1.0856\n",
      "Epoch 24/25\n",
      "1095/1095 [==============================] - 46s 42ms/step - loss: 1.0740\n",
      "Epoch 25/25\n",
      "1095/1095 [==============================] - 49s 44ms/step - loss: 1.0629\n",
      "Генерация из посева: lieve theres an atom of meaning in it. the jury all wrote down on their slates, _she_ doesnt believe\n",
      "lieve theres an atom of meaning in it. the jury all wrote down on their slates, _she_ doesnt believe the dormouse should think it was a little to asked at the mouse to cause with a silence, while the ==================================================\n",
      "Итерация #: 1\n",
      "Epoch 1/25\n",
      "1095/1095 [==============================] - 48s 44ms/step - loss: 1.0521\n",
      "Epoch 2/25\n",
      "1095/1095 [==============================] - 47s 43ms/step - loss: 1.0416\n",
      "Epoch 3/25\n",
      "1095/1095 [==============================] - 47s 43ms/step - loss: 1.0325\n",
      "Epoch 4/25\n",
      "1095/1095 [==============================] - 47s 43ms/step - loss: 1.0237\n",
      "Epoch 5/25\n",
      "1095/1095 [==============================] - 51s 47ms/step - loss: 1.0162\n",
      "Epoch 6/25\n",
      "1095/1095 [==============================] - 48s 44ms/step - loss: 1.0071\n",
      "Epoch 7/25\n",
      "1095/1095 [==============================] - 47s 43ms/step - loss: 0.9999\n",
      "Epoch 8/25\n",
      "1095/1095 [==============================] - 48s 43ms/step - loss: 0.9926\n",
      "Epoch 9/25\n",
      "1095/1095 [==============================] - 48s 43ms/step - loss: 0.9861\n",
      "Epoch 10/25\n",
      "1095/1095 [==============================] - 47s 43ms/step - loss: 0.9793\n",
      "Epoch 11/25\n",
      "1095/1095 [==============================] - 50s 46ms/step - loss: 0.9746\n",
      "Epoch 12/25\n",
      "1095/1095 [==============================] - 47s 43ms/step - loss: 0.9686\n",
      "Epoch 13/25\n",
      "1095/1095 [==============================] - 47s 43ms/step - loss: 0.9635\n",
      "Epoch 14/25\n",
      "1095/1095 [==============================] - 47s 43ms/step - loss: 0.9584\n",
      "Epoch 15/25\n",
      "1095/1095 [==============================] - 47s 43ms/step - loss: 0.9546\n",
      "Epoch 16/25\n",
      "1095/1095 [==============================] - 50s 45ms/step - loss: 0.9502\n",
      "Epoch 17/25\n",
      "1095/1095 [==============================] - 47s 43ms/step - loss: 0.9453\n",
      "Epoch 18/25\n",
      "1095/1095 [==============================] - 46s 42ms/step - loss: 0.9417\n",
      "Epoch 19/25\n",
      "1095/1095 [==============================] - 46s 42ms/step - loss: 0.9385\n",
      "Epoch 20/25\n",
      "1095/1095 [==============================] - 47s 43ms/step - loss: 0.9337\n",
      "Epoch 21/25\n",
      "1095/1095 [==============================] - 49s 45ms/step - loss: 0.9300\n",
      "Epoch 22/25\n",
      "1095/1095 [==============================] - 52s 47ms/step - loss: 0.9273\n",
      "Epoch 23/25\n",
      "1095/1095 [==============================] - 47s 43ms/step - loss: 0.9245\n",
      "Epoch 24/25\n",
      "1095/1095 [==============================] - 47s 43ms/step - loss: 0.9213\n",
      "Epoch 25/25\n",
      "1095/1095 [==============================] - 48s 44ms/step - loss: 0.9176\n",
      "Генерация из посева: this time she had found her way into a tidy little room with a table in the window, and on it (as sh\n",
      "this time she had found her way into a tidy little room with a table in the window, and on it (as she went on and had been a wondering to herself, and the other little childing and suppose it was a ca==================================================\n",
      "Итерация #: 2\n",
      "Epoch 1/25\n",
      "1095/1095 [==============================] - 46s 42ms/step - loss: 0.9173\n",
      "Epoch 2/25\n",
      "1095/1095 [==============================] - 50s 46ms/step - loss: 0.9120\n",
      "Epoch 3/25\n",
      "1095/1095 [==============================] - 47s 43ms/step - loss: 0.9089\n",
      "Epoch 4/25\n",
      "1095/1095 [==============================] - 47s 43ms/step - loss: 0.9072\n",
      "Epoch 5/25\n",
      "1095/1095 [==============================] - 46s 42ms/step - loss: 0.9046\n",
      "Epoch 6/25\n",
      "1095/1095 [==============================] - 46s 42ms/step - loss: 0.9021\n",
      "Epoch 7/25\n",
      "1095/1095 [==============================] - 46s 42ms/step - loss: 0.9003\n",
      "Epoch 8/25\n",
      "1095/1095 [==============================] - 49s 45ms/step - loss: 0.8974\n",
      "Epoch 9/25\n",
      "1095/1095 [==============================] - 47s 43ms/step - loss: 0.8958\n",
      "Epoch 10/25\n",
      "1095/1095 [==============================] - 48s 43ms/step - loss: 0.8941\n",
      "Epoch 11/25\n",
      "1095/1095 [==============================] - 47s 43ms/step - loss: 0.8928\n",
      "Epoch 12/25\n",
      "1095/1095 [==============================] - 47s 43ms/step - loss: 0.8907\n",
      "Epoch 13/25\n",
      "1095/1095 [==============================] - 50s 46ms/step - loss: 0.8888\n",
      "Epoch 14/25\n",
      "1095/1095 [==============================] - 48s 44ms/step - loss: 0.8870\n",
      "Epoch 15/25\n",
      "1095/1095 [==============================] - 47s 43ms/step - loss: 0.8854\n",
      "Epoch 16/25\n",
      "1095/1095 [==============================] - 48s 43ms/step - loss: 0.8820\n",
      "Epoch 17/25\n",
      "1095/1095 [==============================] - 48s 43ms/step - loss: 0.8818\n",
      "Epoch 18/25\n",
      "1095/1095 [==============================] - 48s 44ms/step - loss: 0.8807\n",
      "Epoch 19/25\n",
      "1095/1095 [==============================] - 51s 47ms/step - loss: 0.8780\n",
      "Epoch 20/25\n",
      "1095/1095 [==============================] - 47s 43ms/step - loss: 0.8783\n",
      "Epoch 21/25\n",
      "1095/1095 [==============================] - 51s 46ms/step - loss: 0.8757\n",
      "Epoch 22/25\n",
      "1095/1095 [==============================] - 49s 45ms/step - loss: 0.8761\n",
      "Epoch 23/25\n",
      "1095/1095 [==============================] - 48s 44ms/step - loss: 0.8736\n",
      "Epoch 24/25\n",
      "1095/1095 [==============================] - 51s 46ms/step - loss: 0.8710\n",
      "Epoch 25/25\n",
      "1095/1095 [==============================] - 50s 45ms/step - loss: 0.8695\n",
      "Генерация из посева: i.    who stole the tarts? chapter xii.   alices evidence chapter i. down the rabbit-hole alice was \n",
      "i.    who stole the tarts? chapter xii.   alices evidence chapter i. down the rabbit-hole alice was not a great call him. i dont know when i getting one will make out of the while, his hear his should==================================================\n",
      "Итерация #: 3\n",
      "Epoch 1/25\n",
      "1095/1095 [==============================] - 47s 43ms/step - loss: 0.8691\n",
      "Epoch 2/25\n",
      "1095/1095 [==============================] - 48s 44ms/step - loss: 0.8683\n",
      "Epoch 3/25\n",
      "1095/1095 [==============================] - 47s 43ms/step - loss: 0.8663\n",
      "Epoch 4/25\n",
      "1095/1095 [==============================] - 50s 46ms/step - loss: 0.8661\n",
      "Epoch 5/25\n",
      "1095/1095 [==============================] - 47s 43ms/step - loss: 0.8639\n",
      "Epoch 6/25\n",
      "1095/1095 [==============================] - 47s 43ms/step - loss: 0.8621\n",
      "Epoch 7/25\n",
      "1095/1095 [==============================] - 48s 43ms/step - loss: 0.8621\n",
      "Epoch 8/25\n",
      "1095/1095 [==============================] - 47s 43ms/step - loss: 0.8610\n",
      "Epoch 9/25\n",
      "1095/1095 [==============================] - 48s 44ms/step - loss: 0.8590\n",
      "Epoch 10/25\n",
      "1095/1095 [==============================] - 51s 47ms/step - loss: 0.8582\n",
      "Epoch 11/25\n",
      "1095/1095 [==============================] - 48s 43ms/step - loss: 0.8578\n",
      "Epoch 12/25\n",
      "1095/1095 [==============================] - 48s 44ms/step - loss: 0.8566\n",
      "Epoch 13/25\n",
      "1095/1095 [==============================] - 48s 44ms/step - loss: 0.8567\n",
      "Epoch 14/25\n",
      "1095/1095 [==============================] - 48s 44ms/step - loss: 0.8546\n",
      "Epoch 15/25\n",
      "1095/1095 [==============================] - 49s 45ms/step - loss: 0.8534\n",
      "Epoch 16/25\n",
      "1095/1095 [==============================] - 47s 43ms/step - loss: 0.8534\n",
      "Epoch 17/25\n",
      "1095/1095 [==============================] - 47s 43ms/step - loss: 0.8529\n",
      "Epoch 18/25\n",
      "1095/1095 [==============================] - 45s 41ms/step - loss: 0.8518\n",
      "Epoch 19/25\n",
      "1095/1095 [==============================] - 46s 42ms/step - loss: 0.8502\n",
      "Epoch 20/25\n",
      "1095/1095 [==============================] - 45s 41ms/step - loss: 0.8494\n",
      "Epoch 21/25\n",
      "1095/1095 [==============================] - 50s 45ms/step - loss: 0.8485\n",
      "Epoch 22/25\n",
      "1095/1095 [==============================] - 47s 43ms/step - loss: 0.8475\n",
      "Epoch 23/25\n",
      "1095/1095 [==============================] - 46s 42ms/step - loss: 0.8463\n",
      "Epoch 24/25\n",
      "1095/1095 [==============================] - 46s 42ms/step - loss: 0.8453\n",
      "Epoch 25/25\n",
      "1095/1095 [==============================] - 46s 42ms/step - loss: 0.8460\n",
      "Генерация из посева: ng to see its meaning. and just as id taken the highest tree in the wood, continued the pigeon, rais\n",
      "ng to see its meaning. and just as id taken the highest tree in the wood, continued the pigeon, raising it a fish cault of the trees offer said to herself, and welking it noting about her child of the==================================================\n",
      "Итерация #: 4\n",
      "Epoch 1/25\n",
      "1095/1095 [==============================] - 49s 45ms/step - loss: 0.8447\n",
      "Epoch 2/25\n",
      "1095/1095 [==============================] - 46s 42ms/step - loss: 0.8440\n",
      "Epoch 3/25\n",
      "1095/1095 [==============================] - 47s 43ms/step - loss: 0.8425\n",
      "Epoch 4/25\n",
      "1095/1095 [==============================] - 46s 42ms/step - loss: 0.8409\n",
      "Epoch 5/25\n",
      "1095/1095 [==============================] - 45s 41ms/step - loss: 0.8423\n",
      "Epoch 6/25\n",
      "1095/1095 [==============================] - 45s 41ms/step - loss: 0.8409\n",
      "Epoch 7/25\n",
      "1095/1095 [==============================] - 49s 45ms/step - loss: 0.8392\n",
      "Epoch 8/25\n",
      "1095/1095 [==============================] - 46s 42ms/step - loss: 0.8381\n",
      "Epoch 9/25\n",
      "1095/1095 [==============================] - 46s 42ms/step - loss: 0.8386\n",
      "Epoch 10/25\n",
      "1095/1095 [==============================] - 47s 43ms/step - loss: 0.8371\n",
      "Epoch 11/25\n",
      "1095/1095 [==============================] - 48s 44ms/step - loss: 0.8347\n",
      "Epoch 12/25\n",
      "1095/1095 [==============================] - 46s 42ms/step - loss: 0.8349\n",
      "Epoch 13/25\n",
      "1095/1095 [==============================] - 46s 42ms/step - loss: 0.8347\n",
      "Epoch 14/25\n",
      "1095/1095 [==============================] - 47s 43ms/step - loss: 0.8350\n",
      "Epoch 15/25\n",
      "1095/1095 [==============================] - 49s 45ms/step - loss: 0.8334\n",
      "Epoch 16/25\n",
      "1095/1095 [==============================] - 47s 42ms/step - loss: 0.8320\n",
      "Epoch 17/25\n",
      "1095/1095 [==============================] - 46s 42ms/step - loss: 0.8311\n",
      "Epoch 18/25\n",
      "1095/1095 [==============================] - 49s 45ms/step - loss: 0.8305\n",
      "Epoch 19/25\n",
      "1095/1095 [==============================] - 45s 41ms/step - loss: 0.8305\n",
      "Epoch 20/25\n",
      "1095/1095 [==============================] - 47s 43ms/step - loss: 0.8318\n",
      "Epoch 21/25\n",
      "1095/1095 [==============================] - 48s 44ms/step - loss: 0.8306\n",
      "Epoch 22/25\n",
      "1095/1095 [==============================] - 50s 45ms/step - loss: 0.8304\n",
      "Epoch 23/25\n",
      "1095/1095 [==============================] - 47s 43ms/step - loss: 0.8296\n",
      "Epoch 24/25\n",
      "1095/1095 [==============================] - 48s 44ms/step - loss: 0.8292\n",
      "Epoch 25/25\n",
      "1095/1095 [==============================] - 48s 44ms/step - loss: 0.8278\n",
      "Генерация из посева: ! said the lory, with a shiver. i beg your pardon! said the mouse, frowning, but very politely: did \n",
      "! said the lory, with a shiver. i beg your pardon! said the mouse, frowning, but very politely: did you see. you know what they looked at the mock turtle. alice could think of course, the mock turtle \n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(\n",
    "    GRU(  # вы можете изменить эту часть на LSTM или SimpleRNN, чтобы попробовать альтернативы\n",
    "        HIDDEN_SIZE,\n",
    "        return_sequences=False,\n",
    "        input_shape=(SEQLEN, nb_chars),\n",
    "        unroll=True\n",
    "    )\n",
    ")\n",
    "model.add(Dense(nb_chars))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")\n",
    "\n",
    "\n",
    "# выполнение серий тренировочных и демонстрационных итераций \n",
    "for iteration in range(NUM_ITERATIONS):\n",
    "\n",
    "    # для каждой итерации запуск передачи данных в модель \n",
    "    print(\"=\" * 50)\n",
    "    print(\"Итерация #: %d\" % (iteration))\n",
    "    model.fit(X, y, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS_PER_ITERATION)\n",
    "\n",
    "    # Select a random example input sequence.\n",
    "    test_idx = np.random.randint(len(input_chars))\n",
    "    test_chars = input_chars[test_idx]\n",
    "\n",
    "    # для числа шагов предсказаний использование текущей тренируемой модели \n",
    "    # конструирование one-hot encoding для тестирования input и добавление предсказания.\n",
    "    print(\"Генерация из посева: %s\" % (test_chars))\n",
    "    print(test_chars, end=\"\")\n",
    "    for i in range(NUM_PREDS_PER_EPOCH):\n",
    "\n",
    "        # здесь one-hot encoding.\n",
    "        X_test = np.zeros((1, SEQLEN, nb_chars))\n",
    "        for j, ch in enumerate(test_chars):\n",
    "            X_test[0, j, char2index[ch]] = 1\n",
    "        \n",
    "        # осуществление предсказания с помощью текущей модели.\n",
    "        pred = model.predict(X_test, verbose=0)[0]\n",
    "        y_pred = index2char[np.argmax(pred)]\n",
    "\n",
    "        # вывод предсказания добавленного к тестовому примеру \n",
    "        print(y_pred, end=\"\")\n",
    "\n",
    "        # инкрементация тестового примера содержащего предсказание\n",
    "        test_chars = test_chars[1:] + y_pred\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "1pOlXl04YVIr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Alina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "919"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = sent_tokenize(text)\n",
    "len(max(sentences, key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84.05507955936352"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum( map(len, sentences) ) / len(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_list = list(map(list, sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in sentences_list:\n",
    "    for i, ch in enumerate(sent):\n",
    "        sent[i] = char2index[ch]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTLEN, SEQLEN = 100, 10\n",
    "X_padded = []\n",
    "Y_padded = []\n",
    "for sent in sentences_list:\n",
    "    for i in range(len(sent)-SEQLEN-1):\n",
    "        x = sent[i:(i+SEQLEN)]\n",
    "        #x = [0] * (SEQLEN - len(x)) + x\n",
    "        X_padded.append(x)\n",
    "        Y_padded.append(sent[i+SEQLEN])\n",
    "        #print(x)\n",
    "\n",
    "X = np.zeros((len(X_padded), SEQLEN, nb_chars), dtype='float32')\n",
    "y = np.zeros((len(X_padded), nb_chars), dtype='float32')\n",
    "\n",
    "for i, input_char in enumerate(X_padded):\n",
    "    for j, ch in enumerate(input_char):\n",
    "        X[i, j, ch] = 1\n",
    "    y[i, Y_padded[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "3741/3741 [==============================] - 22s 5ms/step - loss: 2.4931\n",
      "Epoch 2/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 2.1183\n",
      "Epoch 3/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 1.9797\n",
      "Epoch 4/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 1.8806\n",
      "Epoch 5/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 1.8052\n",
      "Epoch 6/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 1.7438\n",
      "Epoch 7/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 1.6913\n",
      "Epoch 8/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 1.6449\n",
      "Epoch 9/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 1.6043\n",
      "Epoch 10/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 1.5681\n",
      "Epoch 11/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 1.5339\n",
      "Epoch 12/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 1.5036\n",
      "Epoch 13/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 1.4754\n",
      "Epoch 14/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 1.4487\n",
      "Epoch 15/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 1.4240\n",
      "Epoch 16/150\n",
      "3741/3741 [==============================] - 19s 5ms/step - loss: 1.4017\n",
      "Epoch 17/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 1.3798\n",
      "Epoch 18/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 1.3596\n",
      "Epoch 19/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 1.3407\n",
      "Epoch 20/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 1.3224\n",
      "Epoch 21/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 1.3050\n",
      "Epoch 22/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 1.2885\n",
      "Epoch 23/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 1.2727\n",
      "Epoch 24/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 1.2576\n",
      "Epoch 25/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 1.2429\n",
      "Epoch 26/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 1.2288\n",
      "Epoch 27/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 1.2154\n",
      "Epoch 28/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 1.2025\n",
      "Epoch 29/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 1.1898\n",
      "Epoch 30/150\n",
      "3741/3741 [==============================] - 19s 5ms/step - loss: 1.1778\n",
      "Epoch 31/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 1.1654\n",
      "Epoch 32/150\n",
      "3741/3741 [==============================] - 19s 5ms/step - loss: 1.1542\n",
      "Epoch 33/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 1.1431\n",
      "Epoch 34/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 1.1316\n",
      "Epoch 35/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 1.1218\n",
      "Epoch 36/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 1.1102\n",
      "Epoch 37/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 1.1004\n",
      "Epoch 38/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 1.0912\n",
      "Epoch 39/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 1.0805\n",
      "Epoch 40/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 1.0713\n",
      "Epoch 41/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 1.0612\n",
      "Epoch 42/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 1.0522\n",
      "Epoch 43/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 1.0427\n",
      "Epoch 44/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 1.0348\n",
      "Epoch 45/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 1.0254\n",
      "Epoch 46/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 1.0169\n",
      "Epoch 47/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 1.0085\n",
      "Epoch 48/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 1.0003\n",
      "Epoch 49/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.9924\n",
      "Epoch 50/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.9838\n",
      "Epoch 51/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.9758\n",
      "Epoch 52/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.9683\n",
      "Epoch 53/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.9611\n",
      "Epoch 54/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.9537\n",
      "Epoch 55/150\n",
      "3741/3741 [==============================] - 19s 5ms/step - loss: 0.9461\n",
      "Epoch 56/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.9388\n",
      "Epoch 57/150\n",
      "3741/3741 [==============================] - 19s 5ms/step - loss: 0.9313\n",
      "Epoch 58/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.9240\n",
      "Epoch 59/150\n",
      "3741/3741 [==============================] - 19s 5ms/step - loss: 0.9173\n",
      "Epoch 60/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.9100\n",
      "Epoch 61/150\n",
      "3741/3741 [==============================] - 19s 5ms/step - loss: 0.9031\n",
      "Epoch 62/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.8965\n",
      "Epoch 63/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.8900\n",
      "Epoch 64/150\n",
      "3741/3741 [==============================] - 19s 5ms/step - loss: 0.8834\n",
      "Epoch 65/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.8774\n",
      "Epoch 66/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.8713\n",
      "Epoch 67/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.8641\n",
      "Epoch 68/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.8589\n",
      "Epoch 69/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.8516\n",
      "Epoch 70/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.8461\n",
      "Epoch 71/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.8405\n",
      "Epoch 72/150\n",
      "3741/3741 [==============================] - 19s 5ms/step - loss: 0.8342\n",
      "Epoch 73/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.8289\n",
      "Epoch 74/150\n",
      "3741/3741 [==============================] - 19s 5ms/step - loss: 0.8228\n",
      "Epoch 75/150\n",
      "3741/3741 [==============================] - 19s 5ms/step - loss: 0.8174\n",
      "Epoch 76/150\n",
      "3741/3741 [==============================] - 19s 5ms/step - loss: 0.8123\n",
      "Epoch 77/150\n",
      "3741/3741 [==============================] - 19s 5ms/step - loss: 0.8057\n",
      "Epoch 78/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.8012\n",
      "Epoch 79/150\n",
      "3741/3741 [==============================] - 19s 5ms/step - loss: 0.7957\n",
      "Epoch 80/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.7904\n",
      "Epoch 81/150\n",
      "3741/3741 [==============================] - 19s 5ms/step - loss: 0.7854\n",
      "Epoch 82/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.7793\n",
      "Epoch 83/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.7739\n",
      "Epoch 84/150\n",
      "3741/3741 [==============================] - 19s 5ms/step - loss: 0.7702\n",
      "Epoch 85/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.7652\n",
      "Epoch 86/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.7598\n",
      "Epoch 87/150\n",
      "3741/3741 [==============================] - 19s 5ms/step - loss: 0.7553\n",
      "Epoch 88/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.7502\n",
      "Epoch 89/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.7457\n",
      "Epoch 90/150\n",
      "3741/3741 [==============================] - 19s 5ms/step - loss: 0.7411\n",
      "Epoch 91/150\n",
      "3741/3741 [==============================] - 19s 5ms/step - loss: 0.7371\n",
      "Epoch 92/150\n",
      "3741/3741 [==============================] - 19s 5ms/step - loss: 0.7321\n",
      "Epoch 93/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.7276\n",
      "Epoch 94/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.7222\n",
      "Epoch 95/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.7182\n",
      "Epoch 96/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.7145\n",
      "Epoch 97/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.7100\n",
      "Epoch 98/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.7062\n",
      "Epoch 99/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.7018\n",
      "Epoch 100/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.6971\n",
      "Epoch 101/150\n",
      "3741/3741 [==============================] - 19s 5ms/step - loss: 0.6934\n",
      "Epoch 102/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.6890\n",
      "Epoch 103/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.6857\n",
      "Epoch 104/150\n",
      "3741/3741 [==============================] - 19s 5ms/step - loss: 0.6817\n",
      "Epoch 105/150\n",
      "3741/3741 [==============================] - 19s 5ms/step - loss: 0.6778\n",
      "Epoch 106/150\n",
      "3741/3741 [==============================] - 19s 5ms/step - loss: 0.6747\n",
      "Epoch 107/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.6703\n",
      "Epoch 108/150\n",
      "3741/3741 [==============================] - 19s 5ms/step - loss: 0.6672\n",
      "Epoch 109/150\n",
      "3741/3741 [==============================] - 19s 5ms/step - loss: 0.6630\n",
      "Epoch 110/150\n",
      "3741/3741 [==============================] - 19s 5ms/step - loss: 0.6591\n",
      "Epoch 111/150\n",
      "3741/3741 [==============================] - 19s 5ms/step - loss: 0.6558\n",
      "Epoch 112/150\n",
      "3741/3741 [==============================] - 19s 5ms/step - loss: 0.6526\n",
      "Epoch 113/150\n",
      "3741/3741 [==============================] - 19s 5ms/step - loss: 0.6494\n",
      "Epoch 114/150\n",
      "3741/3741 [==============================] - 19s 5ms/step - loss: 0.6452\n",
      "Epoch 115/150\n",
      "3741/3741 [==============================] - 19s 5ms/step - loss: 0.6428\n",
      "Epoch 116/150\n",
      "3741/3741 [==============================] - 19s 5ms/step - loss: 0.6395\n",
      "Epoch 117/150\n",
      "3741/3741 [==============================] - 19s 5ms/step - loss: 0.6354\n",
      "Epoch 118/150\n",
      "3741/3741 [==============================] - 19s 5ms/step - loss: 0.6327\n",
      "Epoch 119/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.6299\n",
      "Epoch 120/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.6262\n",
      "Epoch 121/150\n",
      "3741/3741 [==============================] - 19s 5ms/step - loss: 0.6231\n",
      "Epoch 122/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.6196\n",
      "Epoch 123/150\n",
      "3741/3741 [==============================] - 19s 5ms/step - loss: 0.6171\n",
      "Epoch 124/150\n",
      "3741/3741 [==============================] - 19s 5ms/step - loss: 0.6137\n",
      "Epoch 125/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.6103\n",
      "Epoch 126/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.6076\n",
      "Epoch 127/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.6043\n",
      "Epoch 128/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.6014\n",
      "Epoch 129/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.5997\n",
      "Epoch 130/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.5962\n",
      "Epoch 131/150\n",
      "3741/3741 [==============================] - 19s 5ms/step - loss: 0.5933\n",
      "Epoch 132/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.5912\n",
      "Epoch 133/150\n",
      "3741/3741 [==============================] - 19s 5ms/step - loss: 0.5874\n",
      "Epoch 134/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.5858\n",
      "Epoch 135/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.5829\n",
      "Epoch 136/150\n",
      "3741/3741 [==============================] - 19s 5ms/step - loss: 0.5799\n",
      "Epoch 137/150\n",
      "3741/3741 [==============================] - 19s 5ms/step - loss: 0.5769\n",
      "Epoch 138/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.5745\n",
      "Epoch 139/150\n",
      "3741/3741 [==============================] - 19s 5ms/step - loss: 0.5723\n",
      "Epoch 140/150\n",
      "3741/3741 [==============================] - 19s 5ms/step - loss: 0.5702\n",
      "Epoch 141/150\n",
      "3741/3741 [==============================] - 19s 5ms/step - loss: 0.5670\n",
      "Epoch 142/150\n",
      "3741/3741 [==============================] - 19s 5ms/step - loss: 0.5647\n",
      "Epoch 143/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.5630\n",
      "Epoch 144/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.5596\n",
      "Epoch 145/150\n",
      "3741/3741 [==============================] - 19s 5ms/step - loss: 0.5581\n",
      "Epoch 146/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.5559\n",
      "Epoch 147/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.5533\n",
      "Epoch 148/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.5512\n",
      "Epoch 149/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.5493\n",
      "Epoch 150/150\n",
      "3741/3741 [==============================] - 20s 5ms/step - loss: 0.5459\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2e268820a00>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1 = Sequential()\n",
    "model_1.add(LSTM(HIDDEN_SIZE))\n",
    "model_1.add(Dense(nb_chars))\n",
    "model_1.add(Activation(\"softmax\"))\n",
    "model_1.compile(loss=\"categorical_crossentropy\", optimizer=\"adamax\")\n",
    "\n",
    "model_1.fit(X, y, batch_size=32, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: d felt qui\n",
      "te strange at rablingly ready to ask help to stime, very nearly carried it out into the wood seme, t\n",
      "\n",
      "Seed: ther late,\n",
      " and the poor little thing sat down a good in into uped down the chimney this bottle doest to twongh\n",
      "\n",
      "Seed:  mock turt\n",
      "le sighed deeply, and the poor little thing sat down a good in into uped down the chimney this bottl\n",
      "\n",
      "Seed: their putt\n",
      "ing their heads down, as she remembered the words did not seem to draw herehthe heads of the court, \n",
      "\n",
      "Seed:  prove i d\n",
      "idnt know the way of expecting to see it to her feet as the stightyed turter in a very heary off her\n",
      "\n",
      "Seed: ent on and\n",
      " how do the may meat how ravent alice felt a very foor a gount haig to herself, as it we king of a v\n",
      "\n",
      "Seed: way wherev\n",
      "er you go downing to ind the sobs, with a subjer of shaking the paisious shus in a longurious sting,\n",
      "\n",
      "Seed: the house \n",
      "of the mallets live flamingo was gone in a moment to be two peopless so every lone as it this wither\n",
      "\n",
      "Seed:  like the \n",
      "look of the soun, and the poor little thing sat down a good in into uped down the chimney this bottl\n",
      "\n",
      "Seed:  really yo\n",
      "u confusion that she had not a moment to be two peopless so every lone as it this wither their heads\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "\n",
    "    test_idx = np.random.randint(len(X_padded))\n",
    "    test_chars = X_padded[test_idx]\n",
    "\n",
    "    test_str = \"\"\n",
    "    for i in test_chars:\n",
    "        test_str = test_str + index2char[i]\n",
    "\n",
    "    print(f\"Seed: {test_str}\")\n",
    "\n",
    "    for i in range(100):\n",
    "\n",
    "        X_test = np.zeros((1, SEQLEN, nb_chars))\n",
    "        for j, ch in enumerate(test_chars):\n",
    "            X_test[0, j, ch] = 1\n",
    "\n",
    "        pred = model_1.predict(X_test, verbose=0)[0]\n",
    "        ch_idx = np.argmax(pred)\n",
    "        y_pred = index2char[ch_idx]\n",
    "\n",
    "        print(y_pred, end=\"\")\n",
    "\n",
    "        test_chars = test_chars[1:]\n",
    "        test_chars.append(ch_idx)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Для обучения использовался полный текст \"Алисы в стране чудес\".\n",
    "\n",
    "2. Модель обучалась на отрывках проедложений, а не на соучайных помледовательностях из текста.\n",
    "\n",
    "3. Использована модель LSTM и оптимизатор Adamax.\n",
    "\n",
    "4. Уменьшен размер батча и увеличено количество эпох.\n",
    "\n",
    "При доступе к более мощным процессорам GPU можно попробовать изменить следующие параметры:\n",
    "\n",
    "* Сократить batch size до минимума.\n",
    "* Увеличить количесво эпох до нескольких сотен.\n",
    "* Попробовать другое разбиение текста (на более крупные отрезки).\n",
    "* Увеличить количество слоев модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
